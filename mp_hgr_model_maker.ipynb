{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HowTo: MediaPipe Model Training Jupyter Notebook for Custom Model HGRüñêÔ∏è with Gesture Recognition Task \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: [*jk4e*](https://github.com/jk4e)  \n",
    "Date: *Sep. 2024*  \n",
    "File: *mp_model_maker.ipynb*\n",
    "\n",
    "---\n",
    "\n",
    "This Jupyter notebook is designed for local training of a custom hand gesture recognition model. It demonstrates the complete end-to-end process of customizing a gesture recognition model to identify hand gestures using the provided example dataset. This notebook serves as an additional resource to the official Google Colab notebook and guide from the MediaPipe documentation.\n",
    "\n",
    "### Why Train Locally Instead of Using Google Colab?\n",
    "\n",
    "Through my own testing, I found that training on Google Colab is generally 1) unnecessary, and 2) not faster than training locally. This is because the vector embedding of hand keypoints takes 10 times or even longer than the actual model training, and embedding cannot be accelerated with a GPU. \n",
    "\n",
    "If you have a decent CPU with more than 4 cores and enough RAM, local training is likely the better option. Free Google Colab provides only relatively low-performance hardware (2 CPU cores), so the local approach is often faster. Moreover, you don't necessarily need an NVIDIA GPU for neural network training. The model architecture, the small number of images required for training (around 100+ per class), and the low number of epochs can easily be handled by a CPU.\n",
    "\n",
    "In addition to this notebook, you'll find other helpful resources to assist you in getting started.\n",
    "\n",
    "Helpful resources:\n",
    "- Documentation - Offical Guide for Gesture Recognition Task: https://ai.google.dev/edge/mediapipe/solutions/vision/gesture_recognizer\n",
    "- Setup - Setup guide for Python: https://ai.google.dev/edge/mediapipe/solutions/setup_python\n",
    "- Training - Hand gesture recognition model customization guide: https://ai.google.dev/edge/mediapipe/solutions/customization/gesture_recognizer\n",
    "- Inference - Gesture recognition guide for Python: https://ai.google.dev/edge/mediapipe/solutions/vision/gesture_recognizer/python\n",
    "\n",
    "\n",
    "- A Guide for Beginners: https://blog.roboflow.com/what-is-mediapipe/\n",
    "- GitHub: https://github.com/google-ai-edge/mediapipe\n",
    "- API Refernce: https://ai.google.dev/edge/api\n",
    "\n",
    "Google Colab / Jupyter Notebooks:\n",
    "- MP HGR Walkthrough: https://github.com/google-ai-edge/mediapipe-samples/blob/main/tutorials/gesture_recognizer/WiMLS_2022_MediaPipe_Gesture_Recognizer_Walkthrough.ipynb\n",
    "- Inference: https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/gesture_recognizer/python/gesture_recognizer.ipynb\n",
    "- Customization: https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/customization/gesture_recognizer.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites and Installation\n",
    "\n",
    "Before training with this notebook, you'll need to install the necessary Python packages.\n",
    "\n",
    "1. Clone the repository to your local machine.\n",
    "2. Create a virtual environment and activate, e.g., using conda.\n",
    "3. Navigate to the cloned repository's root directory.\n",
    "4. Install the required packages using pip:\n",
    "    ```bash\n",
    "    pip install -r requirements.txt\n",
    "    ```\n",
    "\n",
    "**Note:** Currently, the [MediaPipe Model Maker package](https://pypi.org/project/mediapipe-model-maker/) can only be installed on Linux. If you're using a Silicon-based macOS or Windows, you'll encounter an error. \n",
    "\n",
    "For Windows users, the workaround is to install WSL2 (Windows Subsystem for Linux). Once inside WSL2, you can install the package. If you prefer not to use WSL2 or set up a Linux environment, your only alternative is to use Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlW0fw1t76V2"
   },
   "source": [
    "## Import the required python packages\n",
    "\n",
    "**Note:** Warnings you can typiclly ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7017,
     "status": "ok",
     "timestamp": 1703069482309,
     "user": {
      "displayName": "Jan",
      "userId": "04038293795482087251"
     },
     "user_tz": -60
    },
    "id": "IHGXh1RZ76V3",
    "outputId": "4fdf0c92-6bb0-4135-d834-581cbf104b23"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith(\"2\")\n",
    "\n",
    "from mediapipe_model_maker import gesture_recognizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning Up with Garbage Collector**: To free up resources and improve performance, especially when running the notebook multiple times, it‚Äôs a good practice to manually clean up the RAM using Python's garbage collector. This can help avoid memory issues and ensure smoother performance.   Clean up with Garbage Collector to free resources.\n",
    "\n",
    "**Tip**: It's also a good practice to restart the notebook kernel occasionally to completely reset the environment and clear any lingering resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INFO**: Show you the current workspace path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbNchVoU76V3"
   },
   "source": [
    "## Get the Dataset\n",
    "\n",
    "This notebook demonstrates the complete process of customizing a gesture recognition model for identifying hand gestures using the Rock Paper Scissors (RPS) dataset. For the gesture recognition model in Model Maker, the dataset must follow this directory structure: \n",
    "\n",
    "`<dataset_path>/<label_name>/<img_name>.*`\n",
    "\n",
    "Additionally, one of the label names (`label_name`) must be `none`, which represents any gesture that isn't classified as one of the specific gestures in your dataset.\n",
    "\n",
    "**TODO**: Update the file path to your dataset folder if you are training a custom model on your own dataset.\n",
    "\n",
    "You have two options:\n",
    "1. Download and use the example Rock Paper Scissors dataset.\n",
    "2. Set your own custom dataset path.\n",
    "\n",
    "**Note**: Ensure that your dataset follows the required subfolder structure and includes a `none` folder, even if it's empty. While you can train with an empty `none` folder, it must be present in the directory structure; otherwise, you will encounter an error. \n",
    "\n",
    "It‚Äôs also important to note that omitting the `none` label from your dataset is not recommended, as it can negatively impact your model's accuracy and ability to generalize to unseen gestures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option**: Train with the example dataset (if you train on your own dataset ignore this cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "!wget https://storage.googleapis.com/mediapipe-tasks/gesture_recognizer/rps_data_sample.zip\n",
    "\n",
    "# Define the path to the zip file and the dataset directory\n",
    "zip_file_path = \"rps_data_sample.zip\"\n",
    "dataset_path = \"\"\n",
    "\n",
    "# Unzip the file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(dataset_path)\n",
    "\n",
    "# Verify the contents\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    for file in files:\n",
    "        print(os.path.join(root, file))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option**: Train with your own dataset (set the path to the dataset folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"rps_data_sample\"  # change the dataset path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INFO**: Print dataset path and data labels to verify everthing is working and correct. For the RPS there should be four gesture labels, with one of them being the `none` gesture.\n",
    "\n",
    "You should get the following cell output when training with the example dataset:\n",
    "\n",
    "```\n",
    "rps_data_sample\n",
    " Labels: ['scissors', 'rock', 'none', 'paper'] \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_path)\n",
    "labels = [\n",
    "    i\n",
    "    for i in os.listdir(dataset_path)\n",
    "    if os.path.isdir(os.path.join(dataset_path, i))\n",
    "]\n",
    "print(f\"\\u001b[32m Labels: {labels} \\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h24gljz476V4"
   },
   "source": [
    "## Show Examples of Each Label from the Dataset\n",
    "\n",
    "**INFO:** This section plots a subset of images for each class in the dataset. If your `none` folder is empty, the plots for that label will be blank. Viewing these plots can help you better understand the content and quality of the images in your dataset.\n",
    "\n",
    "The function displays a few example images for each gesture. You can easily adjust the number of examples shown per plot by modifying the `NUM_EXAMPLES = 5` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1vSyiXzb4K-48wgioe-CjuenT3hjvgdug"
    },
    "executionInfo": {
     "elapsed": 40453,
     "status": "ok",
     "timestamp": 1703069542335,
     "user": {
      "displayName": "Jan",
      "userId": "04038293795482087251"
     },
     "user_tz": -60
    },
    "id": "s4gij1gB76V4",
    "outputId": "1a348c2d-fc03-4bb7-e2bd-73156d703566"
   },
   "outputs": [],
   "source": [
    "NUM_EXAMPLES = 5\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Show the images.\n",
    "for label in labels:\n",
    "    label_dir = os.path.join(dataset_path, label)\n",
    "    example_filenames = os.listdir(label_dir)[:NUM_EXAMPLES]\n",
    "    fig, axs = plt.subplots(1, NUM_EXAMPLES, figsize=(10, 2))\n",
    "    for i in range(NUM_EXAMPLES):\n",
    "        try: \n",
    "            axs[i].imshow(plt.imread(os.path.join(label_dir, example_filenames[i])))\n",
    "        except IndexError:\n",
    "            break\n",
    "        axs[i].get_xaxis().set_visible(False)\n",
    "        axs[i].get_yaxis().set_visible(False)\n",
    "    fig.suptitle(f\"Showing {NUM_EXAMPLES} examples for {label}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8gYLBmz76V5"
   },
   "source": [
    "The core training workflow consists of four steps, each separated into its own code block.\n",
    "\n",
    "## Load the Dataset\n",
    "\n",
    "This step involves loading the dataset and performing vector embedding of the hand keypoints. If your dataset is large, this process may take some time.\n",
    "\n",
    "**How It Works:**\n",
    "The dataset is loaded from the `dataset_path` using the `Dataset.from_folder` method. When loading, the pre-packaged hand detection model from MediaPipe Hands detects hand landmarks in the images. Any images without detected hands are omitted from the dataset. The resulting dataset will contain the extracted hand landmark positions from each image, instead of the raw images themselves.\n",
    "\n",
    "### Split the Dataset\n",
    "\n",
    "You can adjust the dataset split as needed. The default split is 80% for training, 10% for validation, and 10% for testing (splitting the remaining data equally between validation and test sets).\n",
    "\n",
    "```python\n",
    "train_data, rest_data = data.split(0.8)\n",
    "validation_data, test_data = rest_data.split(0.5)\n",
    "```\n",
    "\n",
    "### Cell Output\n",
    "\n",
    "Once the dataset is processed, a summary is displayed in the cell output, showing the absolute sizes of the train, validation, and test datasets. Note that during preprocessing, some images may not have detectable hands or extractable keypoints. As a result, the actual data sizes may be slightly lower.\n",
    "\n",
    "Cell output for example dataset:\n",
    "```\n",
    " Number of classes: 4 \n",
    " Train data size: 378 \n",
    " Validation data size: 47 \n",
    " Test data size: 48 \n",
    " ```\n",
    "\n",
    "### Parameters\n",
    "\n",
    "The `HandDataPreprocessingParams` class provides two configurable options for data loading:\n",
    "\n",
    "- **shuffle**: A boolean that controls whether the dataset should be shuffled. Defaults to `True`.\n",
    "- **min_detection_confidence**: A float (0 to 1) that sets the confidence threshold for hand detection. Defaults to `0.7`. A higher value means the detection model must be more confident in identifying a hand, which may improve dataset quality but reduce the number of usable images, as less confident detections are discarded.\n",
    "\n",
    "**Note:** This processing step can take considerable time, so be patient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = gesture_recognizer.Dataset.from_folder(\n",
    "    dirname=dataset_path, hparams=gesture_recognizer.HandDataPreprocessingParams()\n",
    ")\n",
    "\n",
    "train_data, rest_data = data.split(0.8)\n",
    "validation_data, test_data = rest_data.split(0.5)\n",
    "\n",
    "num_of_classes = len(labels)\n",
    "num_of_train_data = len(train_data)\n",
    "num_of_validation_data = len(validation_data)\n",
    "num_of_test_data = len(test_data)\n",
    "\n",
    "print(f\"\\033[35m Number of classes: {num_of_classes} \\033[0m\")\n",
    "print(f\"\\033[35m Train data size: {num_of_train_data} \\033[0m\")\n",
    "print(f\"\\033[35m Validation data size: {num_of_validation_data} \\033[0m\")\n",
    "print(f\"\\033[35m Test data size: {num_of_test_data} \\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iz2e7Xol76V6"
   },
   "source": [
    "## Train a model with the given parameters (standard or custom)\n",
    "\n",
    "Train the custom gesture recognizer by using the create method and passing in the training data, validation data, model options, and hyperparameters. For more information on model options and hyperparameters, see the Hyperparameters section below or take a look in the [Docs](https://ai.google.dev/edge/mediapipe/solutions/customization/gesture_recognizer)/[API Reference](https://ai.google.dev/edge/api/mediapipe/python/mediapipe_model_maker/gesture_recognizer/HParams)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Model\n",
    "\n",
    "Train the model using the default values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Standard setup of model maker --\n",
    "\n",
    "hparams = gesture_recognizer.HParams(export_dir=\"exported_model\")\n",
    "options = gesture_recognizer.GestureRecognizerOptions(hparams=hparams)\n",
    "model = gesture_recognizer.GestureRecognizer.create(\n",
    "    train_data=train_data,\n",
    "    validation_data=validation_data,\n",
    "    options=options\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GU7FPd176V7"
   },
   "source": [
    "## Evaluation of the Model Performance\n",
    "\n",
    "After training the model, evaluate its performance on the test dataset by printing the loss and accuracy metrics. The goal is to achieve a low loss value and a high accuracy score.\n",
    "\n",
    "**Note**: Be cautious when interpreting high test accuracy values. If your model achieves an accuracy of `1.0`, this doesn't necessarily indicate that you‚Äôve built a perfect model. It‚Äôs more likely that your model is overfitting, especially if your dataset is small and lacks variation. The true goal is to build a model with good generalization.\n",
    "\n",
    "To ensure generalization, your dataset should represent a wide range of examples. For instance, it shouldn't only contain images of your own hand, but also those of other people‚Äôs hands (depending on the task and goal of your project).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25159,
     "status": "ok",
     "timestamp": 1703075135138,
     "user": {
      "displayName": "Jan",
      "userId": "04038293795482087251"
     },
     "user_tz": -60
    },
    "id": "RmdSWcdo76V8",
    "outputId": "f210e6d0-ae7e-4c0a-be8c-367dcc21c6b4"
   },
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(test_data, batch_size=1)\n",
    "print(f\"Test loss:{loss}, Test accuracy:{acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the trained model as a Tensorflow Lite Model\n",
    "\n",
    "After creating the model, convert and export it to a Tensorflow Lite model format for later use on an on-device application. The export also includes model metadata, which includes the label file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export_model()\n",
    "!ls exported_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Model On-Device\n",
    "\n",
    "To deploy the TFLite model for on-device usage through MediaPipe Tasks, refer to the Gesture Recognizer [overview page](https://developers.google.com/mediapipe/solutions/vision/gesture_recognizer).\n",
    "\n",
    "You're not limited to using Python; the model can also be integrated into Android, iOS, or Web applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéÆ Playground for Hyperparameter Tuning\n",
    "\n",
    "----\n",
    "\n",
    "You can further customize the model using the `GestureRecognizerOptions` class, which has two optional parameters for `ModelOptions` and `HParams`. Use the `ModelOptions` class to customize parameters related to the model itself, and the `HParams` class to customize other parameters related to training and saving the model.\n",
    "\n",
    "`ModelOptions` has one customizable parameter that affects accuracy:\n",
    "* `dropout_rate`: The fraction of the input units to drop. Used in dropout layer. Defaults to 0.05.\n",
    "* `layer_widths`: A list of hidden layer widths for the gesture model. Each element in the list will create a new hidden layer with the specified width. The hidden layers are separated with BatchNorm, Dropout, and ReLU. Defaults to an empty list(no hidden layers).\n",
    "\n",
    "`HParams` has the following list of customizable parameters which affect model accuracy:\n",
    "* `learning_rate`: The learning rate to use for gradient descent training. Defaults to 0.001.\n",
    "* `batch_size`: Batch size for training. Defaults to 2.\n",
    "* `epochs`: Number of training iterations over the dataset. Defaults to 10.\n",
    "* `steps_per_epoch`: An optional integer that indicates the number of training steps per epoch. If not set, the training pipeline calculates the default steps per epoch as the training dataset size divided by batch size.\n",
    "* `shuffle`: True if the dataset is shuffled before training. Defaults to False.\n",
    "* `lr_decay`: Learning rate decay to use for gradient descent training. Defaults to 0.99.\n",
    "* `gamma`: Gamma parameter for focal loss. Defaults to 2\n",
    "\n",
    "Additional `HParams` parameter that does not affect model accuracy:\n",
    "* `export_dir`: The location of the model checkpoint files and exported model files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Model (Just an example, see above Adjustable parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = gesture_recognizer.HParams(learning_rate=0.003, export_dir=\"exported_model_2\")\n",
    "model_options = gesture_recognizer.ModelOptions(dropout_rate=0.2)\n",
    "options = gesture_recognizer.GestureRecognizerOptions(model_options=model_options, hparams=hparams)\n",
    "model_2 = gesture_recognizer.GestureRecognizer.create(\n",
    "    train_data=train_data,\n",
    "    validation_data=validation_data,\n",
    "    options=options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model_2.evaluate(test_data)\n",
    "print(f\"Test loss:{loss}, Test accuracy:{accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.export_model()\n",
    "!ls exported_model"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
